{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard data analytical libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import os, warnings, time, dmba\n",
    "import scikitplot as skplt \n",
    "\n",
    "#Data Mining Book Libraries\n",
    "from dmba import liftChart, gainsChart,regressionSummary, classificationSummary, exhaustive_search\n",
    "from dmba import backward_elimination, forward_selection, stepwise_selection, adjusted_r2_score, AIC_score, BIC_score\n",
    "from os.path import exists\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_curve, auc, roc_auc_score, plot_confusion_matrix,confusion_matrix,r2_score\n",
    "#Classification \n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression,  LinearRegression, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, kneighbors_graph\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Used to save keystrokes when wanting to print something. Now we can just use\n",
    "# p(\"Hello\") instead of print(\"Hello\")\n",
    "p = print\n",
    "# import csv\n",
    "# import re\n",
    "\n",
    "# Change this value if you are not using o_desktop\n",
    "computer = 'o_desktop'\n",
    "#computer = 'other'\n",
    "if (computer == 'o_desktop'):\n",
    "    os.environ['NUMEXPR_MAX_THREADS'] = '24'\n",
    "else:\n",
    "    # default is 4 or 8\n",
    "    os.environ['NUMEXPR_MAX_THREADS'] = '8'\n",
    "\n",
    "# For future use:\n",
    "# import threading\n",
    "# import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>respondent_id</th>\n",
       "      <th>h1n1_concern</th>\n",
       "      <th>h1n1_knowledge</th>\n",
       "      <th>behavioral_antiviral_meds</th>\n",
       "      <th>behavioral_avoidance</th>\n",
       "      <th>behavioral_face_mask</th>\n",
       "      <th>behavioral_wash_hands</th>\n",
       "      <th>behavioral_large_gatherings</th>\n",
       "      <th>behavioral_outside_home</th>\n",
       "      <th>behavioral_touch_face</th>\n",
       "      <th>...</th>\n",
       "      <th>rent_or_own</th>\n",
       "      <th>employment_status</th>\n",
       "      <th>hhs_geo_region</th>\n",
       "      <th>census_msa</th>\n",
       "      <th>household_adults</th>\n",
       "      <th>household_children</th>\n",
       "      <th>employment_industry</th>\n",
       "      <th>employment_occupation</th>\n",
       "      <th>h1n1_vaccine</th>\n",
       "      <th>seasonal_vaccine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Own</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>oxchjgsf</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Employed</td>\n",
       "      <td>bhuqouqj</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>pxcmvdjn</td>\n",
       "      <td>xgwztkwe</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rucpziij</td>\n",
       "      <td>xtkaffoo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Rent</td>\n",
       "      <td>Not in Labor Force</td>\n",
       "      <td>lrircsnp</td>\n",
       "      <td>MSA, Principle City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Own</td>\n",
       "      <td>Employed</td>\n",
       "      <td>qufhixun</td>\n",
       "      <td>MSA, Not Principle  City</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wxleyezf</td>\n",
       "      <td>emcorrxb</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   respondent_id  h1n1_concern  h1n1_knowledge  behavioral_antiviral_meds  \\\n",
       "0              0           1.0             0.0                        0.0   \n",
       "1              1           3.0             2.0                        0.0   \n",
       "2              2           1.0             1.0                        0.0   \n",
       "3              3           1.0             1.0                        0.0   \n",
       "4              4           2.0             1.0                        0.0   \n",
       "\n",
       "   behavioral_avoidance  behavioral_face_mask  behavioral_wash_hands  \\\n",
       "0                   0.0                   0.0                    0.0   \n",
       "1                   1.0                   0.0                    1.0   \n",
       "2                   1.0                   0.0                    0.0   \n",
       "3                   1.0                   0.0                    1.0   \n",
       "4                   1.0                   0.0                    1.0   \n",
       "\n",
       "   behavioral_large_gatherings  behavioral_outside_home  \\\n",
       "0                          0.0                      1.0   \n",
       "1                          0.0                      1.0   \n",
       "2                          0.0                      0.0   \n",
       "3                          1.0                      0.0   \n",
       "4                          1.0                      0.0   \n",
       "\n",
       "   behavioral_touch_face  ...  rent_or_own   employment_status  \\\n",
       "0                    1.0  ...          Own  Not in Labor Force   \n",
       "1                    1.0  ...         Rent            Employed   \n",
       "2                    0.0  ...          Own            Employed   \n",
       "3                    0.0  ...         Rent  Not in Labor Force   \n",
       "4                    1.0  ...          Own            Employed   \n",
       "\n",
       "   hhs_geo_region                census_msa  household_adults  \\\n",
       "0        oxchjgsf                   Non-MSA               0.0   \n",
       "1        bhuqouqj  MSA, Not Principle  City               0.0   \n",
       "2        qufhixun  MSA, Not Principle  City               2.0   \n",
       "3        lrircsnp       MSA, Principle City               0.0   \n",
       "4        qufhixun  MSA, Not Principle  City               1.0   \n",
       "\n",
       "   household_children  employment_industry  employment_occupation  \\\n",
       "0                 0.0                  NaN                    NaN   \n",
       "1                 0.0             pxcmvdjn               xgwztkwe   \n",
       "2                 0.0             rucpziij               xtkaffoo   \n",
       "3                 0.0                  NaN                    NaN   \n",
       "4                 0.0             wxleyezf               emcorrxb   \n",
       "\n",
       "   h1n1_vaccine  seasonal_vaccine  \n",
       "0             0                 0  \n",
       "1             0                 1  \n",
       "2             0                 0  \n",
       "3             0                 1  \n",
       "4             0                 0  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting directories and loading training set and training labels\n",
    "repo_directory = r'C:/ADS_599_Final/'\n",
    "data_folder_directory = r'C:/ADS_599_Final/Data_Folder/'\n",
    "df_features_file = 'C:/ADS_599_Final/Data_Folder/training_set_features.csv'\n",
    "df_labels_file = 'C:/ADS_599_Final/Data_Folder/training_set_labels.csv'\n",
    "df = pd.read_csv(df_features_file)\n",
    "df_labels = pd.read_csv(df_labels_file)\n",
    "\n",
    "# Combining training data with training labels for modeling\n",
    "df = df.join(df_labels.set_index('respondent_id'), on='respondent_id')\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Null Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace NaNs with a new value. A person refusing to answer a question could be significant. Note: for imputations we will have to put back the NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming df\n",
    "df_train = df\n",
    "\n",
    "# Categories\n",
    "df_train['h1n1_concern'] = df_train['h1n1_concern'].fillna(-1)\n",
    "df_train['h1n1_knowledge'] = df_train['h1n1_knowledge'].fillna(-1)\n",
    "df_train['behavioral_antiviral_meds'] = df_train['behavioral_antiviral_meds'].fillna(-1)\n",
    "df_train['behavioral_avoidance'] = df_train['behavioral_avoidance'].fillna(-1)\n",
    "df_train['behavioral_face_mask'] = df_train['behavioral_face_mask'].fillna(-1)\n",
    "df_train['behavioral_large_gatherings'] = df_train['behavioral_large_gatherings'].fillna(-1)\n",
    "df_train['behavioral_outside_home'] = df_train['behavioral_outside_home'].fillna(-1)\n",
    "df_train['behavioral_wash_hands'] = df_train['behavioral_wash_hands'].fillna(-1)          \n",
    "df_train['behavioral_touch_face'] = df_train['behavioral_touch_face'].fillna(-1)\n",
    "df_train['doctor_recc_h1n1'] = df_train['doctor_recc_h1n1'].fillna(-1)\n",
    "df_train['doctor_recc_seasonal'] = df_train['doctor_recc_seasonal'].fillna(-1)\n",
    "df_train['chronic_med_condition'] = df_train['chronic_med_condition'].fillna(-1)\n",
    "df_train['child_under_6_months'] = df_train['child_under_6_months'].fillna(-1)\n",
    "df_train['health_worker'] = df_train['health_worker'].fillna(-1)\n",
    "df_train['health_insurance'] = df_train['health_insurance'].fillna(-1)\n",
    "df_train['opinion_h1n1_vacc_effective'] = df_train['opinion_h1n1_vacc_effective'].fillna(-1)\n",
    "df_train['opinion_h1n1_sick_from_vacc'] = df_train['opinion_h1n1_sick_from_vacc'].fillna(-1)\n",
    "df_train['opinion_h1n1_risk'] = df_train['opinion_h1n1_risk'].fillna(-1)\n",
    "df_train['opinion_seas_vacc_effective'] = df_train['opinion_seas_vacc_effective'].fillna(-1)\n",
    "df_train['opinion_seas_risk'] = df_train['opinion_seas_risk'].fillna(-1)\n",
    "df_train['opinion_seas_sick_from_vacc'] = df_train['opinion_seas_sick_from_vacc'].fillna(-1)\n",
    "df_train['household_adults'] = df_train['household_adults'].fillna(-1)\n",
    "df_train['household_children'] = df_train['household_children'].fillna(-1)\n",
    "\n",
    "# Numbers\n",
    "df_train['age_group'] = df_train['age_group'].fillna(\"no_response\")\n",
    "df_train['education'] = df_train['education'].fillna(\"no_response\")\n",
    "df_train['race'] = df_train['race'].fillna(\"no_response\")\n",
    "df_train['income_poverty'] = df_train['income_poverty'].fillna(\"no_response\")\n",
    "df_train['marital_status'] = df_train['marital_status'].fillna(\"no_response\")\n",
    "df_train['rent_or_own'] = df_train['rent_or_own'].fillna(\"no_response\")\n",
    "df_train['employment_status'] = df_train['employment_status'].fillna(\"no_response\")\n",
    "df_train['employment_occupation'] = df_train['employment_occupation'].fillna(\"no_response\")\n",
    "df_train['employment_industry'] = df_train['employment_industry'].fillna(\"no_response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting those categories to numbers. Category encoding known in the R world as as.factor-ing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After encoding the null counts per column are: \n",
      "respondent_id                  0\n",
      "h1n1_concern                   0\n",
      "h1n1_knowledge                 0\n",
      "behavioral_antiviral_meds      0\n",
      "behavioral_avoidance           0\n",
      "behavioral_face_mask           0\n",
      "behavioral_wash_hands          0\n",
      "behavioral_large_gatherings    0\n",
      "behavioral_outside_home        0\n",
      "behavioral_touch_face          0\n",
      "doctor_recc_h1n1               0\n",
      "doctor_recc_seasonal           0\n",
      "chronic_med_condition          0\n",
      "child_under_6_months           0\n",
      "health_worker                  0\n",
      "health_insurance               0\n",
      "opinion_h1n1_vacc_effective    0\n",
      "opinion_h1n1_risk              0\n",
      "opinion_h1n1_sick_from_vacc    0\n",
      "opinion_seas_vacc_effective    0\n",
      "opinion_seas_risk              0\n",
      "opinion_seas_sick_from_vacc    0\n",
      "age_group                      0\n",
      "education                      0\n",
      "race                           0\n",
      "sex                            0\n",
      "income_poverty                 0\n",
      "marital_status                 0\n",
      "rent_or_own                    0\n",
      "employment_status              0\n",
      "hhs_geo_region                 0\n",
      "census_msa                     0\n",
      "household_adults               0\n",
      "household_children             0\n",
      "employment_industry            0\n",
      "employment_occupation          0\n",
      "h1n1_vaccine                   0\n",
      "seasonal_vaccine               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Label encoding\n",
    "df_train_label = df_train\n",
    "    # Encode labels the below is equivalent to df_train['hhs_geo_region']= label_encoder.fit_transform(df_train['hhs_geo_region'])\n",
    "df_train_label[\"hhs_geo_region\"] = df_train[\"hhs_geo_region\"].astype('category')\n",
    "df_train_label[\"hhs_geo_region\"] = df_train[\"hhs_geo_region\"].cat.codes\n",
    "df_train_label[\"census_msa\"] = df_train[\"census_msa\"].astype('category')\n",
    "df_train_label[\"census_msa\"] = df_train[\"census_msa\"].cat.codes\n",
    "df_train_label[\"employment_industry\"] = df_train[\"employment_industry\"].astype('category')\n",
    "df_train_label[\"employment_industry\"] = df_train[\"employment_industry\"].cat.codes\n",
    "df_train_label[\"employment_occupation\"] = df_train[\"employment_occupation\"].astype('category')\n",
    "df_train_label[\"employment_occupation\"] = df_train[\"employment_occupation\"].cat.codes\n",
    "df_train_label[\"employment_status\"] = df_train[\"employment_status\"].astype('category')\n",
    "df_train_label[\"employment_status\"] = df_train[\"employment_status\"].cat.codes\n",
    "df_train_label[\"rent_or_own\"] = df_train[\"rent_or_own\"].astype('category')\n",
    "df_train_label[\"rent_or_own\"] = df_train[\"rent_or_own\"].cat.codes\n",
    "df_train_label[\"marital_status\"] = df_train[\"marital_status\"].astype('category')\n",
    "df_train_label[\"marital_status\"] = df_train[\"marital_status\"].cat.codes\n",
    "df_train_label[\"income_poverty\"] = df_train[\"income_poverty\"].astype('category')\n",
    "df_train_label[\"income_poverty\"] = df_train[\"income_poverty\"].cat.codes\n",
    "df_train_label[\"race\"] = df_train[\"race\"].astype('category')\n",
    "df_train_label[\"race\"] = df_train[\"race\"].cat.codes\n",
    "df_train_label[\"education\"] = df_train[\"education\"].astype('category')\n",
    "df_train_label[\"education\"] = df_train[\"education\"].cat.codes\n",
    "df_train_label[\"age_group\"] = df_train[\"age_group\"].astype('category')\n",
    "df_train_label[\"age_group\"] = df_train[\"age_group\"].cat.codes\n",
    "df_train_label[\"sex\"] = df_train[\"sex\"].astype('category')\n",
    "df_train_label[\"sex\"] = df_train[\"sex\"].cat.codes\n",
    "\n",
    "p(\"After encoding the null counts per column are: \")\n",
    "p(df_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling nulls three ways\n",
    "\n",
    "handling_nulls = \"median\" # options \"median\" \"iterative\" \"dropall\"\n",
    "if handling_nulls == \"iterative\":\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train.replace(-1, np.nan) \n",
    "    df_train.replace(\"no_response\", np.nan) \n",
    "    \n",
    "    # SMOTE Sampling\n",
    "    temp_columns = df_train.columns\n",
    "    imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "    imp.fit(df_train)\n",
    "    df_train = pd.DataFrame(data=imp.transform(df_train))\n",
    "    df_train.columns = temp_columns\n",
    "    df_train\n",
    "elif handling_nulls == \"median\":\n",
    "    df_train_median = df_train\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train_median.replace(-1, np.nan) \n",
    "    df_train_median.replace(\"no_response\", np.nan) \n",
    "    df_train_median.fillna(df_train.median())\n",
    "elif handling_nulls == \"dropall\":\n",
    "    df_train_drop = df_train\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train_drop.replace(-1, np.nan) \n",
    "    df_train_drop.replace(\"no_response\", np.nan) \n",
    "    # See how it is if we drop the NaNs\n",
    "    df_train_drop = df_train.dropna(inplace=False) #This should be replace with imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There should be no nulls now: \n",
      "respondent_id                  0\n",
      "h1n1_concern                   0\n",
      "h1n1_knowledge                 0\n",
      "behavioral_antiviral_meds      0\n",
      "behavioral_avoidance           0\n",
      "behavioral_face_mask           0\n",
      "behavioral_wash_hands          0\n",
      "behavioral_large_gatherings    0\n",
      "behavioral_outside_home        0\n",
      "behavioral_touch_face          0\n",
      "doctor_recc_h1n1               0\n",
      "doctor_recc_seasonal           0\n",
      "chronic_med_condition          0\n",
      "child_under_6_months           0\n",
      "health_worker                  0\n",
      "health_insurance               0\n",
      "opinion_h1n1_vacc_effective    0\n",
      "opinion_h1n1_risk              0\n",
      "opinion_h1n1_sick_from_vacc    0\n",
      "opinion_seas_vacc_effective    0\n",
      "opinion_seas_risk              0\n",
      "opinion_seas_sick_from_vacc    0\n",
      "age_group                      0\n",
      "education                      0\n",
      "race                           0\n",
      "sex                            0\n",
      "income_poverty                 0\n",
      "marital_status                 0\n",
      "rent_or_own                    0\n",
      "employment_status              0\n",
      "hhs_geo_region                 0\n",
      "census_msa                     0\n",
      "household_adults               0\n",
      "household_children             0\n",
      "employment_industry            0\n",
      "employment_occupation          0\n",
      "h1n1_vaccine                   0\n",
      "seasonal_vaccine               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "p(\"There should be no nulls now: \")\n",
    "p(df_train_median.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There should be no nulls now: \n",
      "respondent_id                  0\n",
      "h1n1_concern                   0\n",
      "h1n1_knowledge                 0\n",
      "behavioral_antiviral_meds      0\n",
      "behavioral_avoidance           0\n",
      "behavioral_face_mask           0\n",
      "behavioral_wash_hands          0\n",
      "behavioral_large_gatherings    0\n",
      "behavioral_outside_home        0\n",
      "behavioral_touch_face          0\n",
      "doctor_recc_h1n1               0\n",
      "doctor_recc_seasonal           0\n",
      "chronic_med_condition          0\n",
      "child_under_6_months           0\n",
      "health_worker                  0\n",
      "health_insurance               0\n",
      "opinion_h1n1_vacc_effective    0\n",
      "opinion_h1n1_risk              0\n",
      "opinion_h1n1_sick_from_vacc    0\n",
      "opinion_seas_vacc_effective    0\n",
      "opinion_seas_risk              0\n",
      "opinion_seas_sick_from_vacc    0\n",
      "age_group                      0\n",
      "education                      0\n",
      "race                           0\n",
      "sex                            0\n",
      "income_poverty                 0\n",
      "marital_status                 0\n",
      "rent_or_own                    0\n",
      "employment_status              0\n",
      "hhs_geo_region                 0\n",
      "census_msa                     0\n",
      "household_adults               0\n",
      "household_children             0\n",
      "employment_industry            0\n",
      "employment_occupation          0\n",
      "h1n1_vaccine                   0\n",
      "seasonal_vaccine               0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "handling_nulls = \"dropall\"\n",
    "if handling_nulls == \"iterative\":\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train.replace(-1, np.nan) \n",
    "    df_train.replace(\"no_response\", np.nan) \n",
    "    \n",
    "    # SMOTE Sampling\n",
    "    temp_columns = df_train.columns\n",
    "    imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "    imp.fit(df_train)\n",
    "    df_train = pd.DataFrame(data=imp.transform(df_train))\n",
    "    df_train.columns = temp_columns\n",
    "    df_train\n",
    "elif handling_nulls == \"median\":\n",
    "    df_train_median = df_train\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train_median.replace(-1, np.nan) \n",
    "    df_train_median.replace(\"no_response\", np.nan) \n",
    "    df_train_median.fillna(df_train.median())\n",
    "elif handling_nulls == \"dropall\":\n",
    "    df_train_drop = df_train\n",
    "    #Need to add back the NaN for the imputations.\n",
    "    df_train_drop.replace(-1, np.nan) \n",
    "    df_train_drop.replace(\"no_response\", np.nan) \n",
    "    # See how it is if we drop the NaNs\n",
    "    df_train_drop = df_train.dropna(inplace=False) #This should be replace with imputation.\n",
    "p(\"There should be no nulls now: \")\n",
    "p(df_train_drop.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing columns we don't need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping ID as we do not need to use it in the modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Respondent_id are all unique so its irrelevant now that we merged.\n",
    "df_train = df_train.drop(columns=['respondent_id'], inplace=False)\n",
    "df_train_label = df_train_label.drop(columns=['respondent_id'], inplace=False)\n",
    "df_train_median = df_train_median.drop(columns=['respondent_id'], inplace=False)\n",
    "df_train_drop = df_train_drop.drop(columns=['respondent_id'], inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Balancing\n",
    "\n",
    "Balancing/Oversampling minority classes using SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE()\n",
    "\n",
    "# Separating the features and targets\n",
    "# Original Data\n",
    "X_h1n1 = df_train.drop(columns=['h1n1_vaccine', 'seasonal_vaccine'])\n",
    "X_seasonal = X_h1n1\n",
    "y_h1n1 = df_train['h1n1_vaccine']\n",
    "y_seasonal = df_train['seasonal_vaccine']\n",
    "X_h1n1, y_h1n1 = oversample.fit_resample(X_h1n1, y_h1n1)\n",
    "X_seasonal, y_seasonal = oversample.fit_resample(X_seasonal, y_seasonal)\n",
    "\n",
    "# Encoded Data\n",
    "X_label_h1n1 = df_train_label.drop(columns=['h1n1_vaccine', 'seasonal_vaccine'])\n",
    "X_label_seasonal = X_label_h1n1\n",
    "y_label_h1n1 = df_train_label['h1n1_vaccine']\n",
    "y_label_seasonal = df_train_label['seasonal_vaccine']\n",
    "X_label_h1n1, y_label_h1n1 = oversample.fit_resample(X_label_h1n1, y_label_h1n1)\n",
    "X_label_seasonal, y_label_seasonal = oversample.fit_resample(X_label_seasonal, y_label_seasonal)\n",
    "\n",
    "# Nulls replaced with median data\n",
    "X_median_h1n1 = df_train_median.drop(columns=['h1n1_vaccine', 'seasonal_vaccine'])\n",
    "X_median_seasonal = X_median_h1n1\n",
    "y_median_h1n1 = df_train_median['h1n1_vaccine']\n",
    "y_median_seasonal = df_train_median['seasonal_vaccine']\n",
    "X_median_h1n1, y_median_h1n1 = oversample.fit_resample(X_median_h1n1, y_median_h1n1)\n",
    "X_median_seasonal, y_median_seasonal = oversample.fit_resample(X_median_seasonal, y_median_seasonal)\n",
    "\n",
    "# Nulls dropped data\n",
    "X_drop_h1n1 = df_train_drop.drop(columns=['h1n1_vaccine', 'seasonal_vaccine'])\n",
    "X_drop_seasonal = X_drop_h1n1\n",
    "y_drop_h1n1 = df_train_drop['h1n1_vaccine']\n",
    "y_drop_seasonal = df_train_drop['seasonal_vaccine']\n",
    "X_drop_h1n1, y_drop_h1n1 = oversample.fit_resample(X_drop_h1n1, y_drop_h1n1)\n",
    "X_drop_seasonal, y_drop_seasonal = oversample.fit_resample(X_drop_seasonal, y_drop_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h1n1_vaccine\n",
       "0               21033\n",
       "1               21033\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing to see if y is balanced\n",
    "ytest = y_drop_h1n1.to_frame()\n",
    "ytest.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing train/test/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into 70-20-10 train-test-validation sets\n",
    "\n",
    "# Original Data\n",
    "X_train_h1n1, X_test_h1n1, y_train_h1n1, y_test_h1n1 = train_test_split(X_h1n1, y_h1n1, train_size=.7)\n",
    "X_test_h1n1, X_val_h1n1, y_test_h1n1, y_val_h1n1 = train_test_split(X_test_h1n1, y_test_h1n1, train_size=.67)\n",
    "\n",
    "X_train_seasonal, X_test_seasonal, y_train_seasonal, y_test_seasonal = train_test_split(X_seasonal, y_seasonal, train_size=.7)\n",
    "X_test_seasonal, X_val_seasonal, y_test_seasonal, y_val_seasonal = train_test_split(X_test_seasonal, y_test_seasonal, train_size=.67)\n",
    "\n",
    "# Encoded Data\n",
    "X_train_label_h1n1, X_test_label_h1n1, y_train_label_h1n1, y_test_label_h1n1 = train_test_split(X_label_h1n1, y_label_h1n1, train_size=.7)\n",
    "X_test_label_h1n1, X_val_label_h1n1, y_test_label_h1n1, y_val_label_h1n1 = train_test_split(X_test_label_h1n1, y_test_label_h1n1, train_size=.67)\n",
    "\n",
    "X_train_label_seasonal, X_test_label_seasonal, y_train_label_seasonal, y_test_label_seasonal = train_test_split(X_label_seasonal, y_label_seasonal, train_size=.7)\n",
    "X_test_label_seasonal, X_val_label_seasonal, y_test_label_seasonal, y_val_label_seasonal = train_test_split(X_test_label_seasonal, y_test_label_seasonal, train_size=.67)\n",
    "\n",
    "# Nulls replaced with median data\n",
    "X_train_median_h1n1, X_test_median_h1n1, y_train_median_h1n1, y_test_median_h1n1 = train_test_split(X_median_h1n1, y_median_h1n1, train_size=.7)\n",
    "X_test_median_h1n1, X_val_median_h1n1, y_test_median_h1n1, y_val_median_h1n1 = train_test_split(X_test_median_h1n1, y_test_median_h1n1, train_size=.67)\n",
    "\n",
    "X_train_median_seasonal, X_test_median_seasonal, y_train_median_seasonal, y_test_median_seasonal = train_test_split(X_median_seasonal, y_median_seasonal, train_size=.7)\n",
    "X_test_median_seasonal, X_val_median_seasonal, y_test_median_seasonal, y_val_median_seasonal = train_test_split(X_test_median_seasonal, y_test_median_seasonal, train_size=.67)\n",
    "\n",
    "# Nulls dropped data\n",
    "X_train_drop_h1n1, X_test_drop_h1n1, y_train_drop_h1n1, y_test_drop_h1n1 = train_test_split(X_drop_h1n1, y_drop_h1n1, train_size=.7)\n",
    "X_test_drop_h1n1, X_val_drop_h1n1, y_test_drop_h1n1, y_val_drop_h1n1 = train_test_split(X_test_drop_h1n1, y_test_drop_h1n1, train_size=.67)\n",
    "\n",
    "X_train_drop_seasonal, X_test_drop_seasonal, y_train_drop_seasonal, y_test_drop_seasonal = train_test_split(X_drop_seasonal, y_drop_seasonal, train_size=.7)\n",
    "X_test_drop_seasonal, X_val_drop_seasonal, y_test_drop_seasonal, y_val_drop_seasonal = train_test_split(X_test_drop_seasonal, y_test_drop_seasonal, train_size=.67)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalization with sklearn\n",
    "\n",
    "# Fitting a scaler on the training datasets\n",
    "normh1n1= MinMaxScaler().fit(X_train_h1n1)\n",
    "normseasonal = MinMaxScaler().fit(X_train_seasonal)\n",
    "\n",
    "# Transforming the training datasets\n",
    "X_train_norm_h1n1 = normh1n1.transform(X_train_h1n1)\n",
    "X_train_norm_seasonal = normseasonal.transform(X_train_seasonal)\n",
    "\n",
    "# transform the testing dataset\n",
    "X_test_norm_h1n1 = normh1n1.transform(X_test_h1n1)\n",
    "X_test_norm_seasonal = normseasonal.transform(X_test_seasonal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data standardization with sklearn\n",
    "\n",
    "# copy the two datasets\n",
    "X_train_stand_h1n1 = X_train_h1n1.copy()\n",
    "X_train_stand_seasonal = X_train_seasonal.copy()\n",
    "X_test_stand_h1n1 = X_test_h1n1.copy()\n",
    "X_test_stand_seasonal = X_test_seasonal.copy()\n",
    "\n",
    "# Group the numerical features and not categorical\n",
    "\n",
    "num_cols = ['h1n1_concern','h1n1_knowledge','behavioral_antiviral_meds','behavioral_avoidance','behavioral_face_mask','behavioral_large_gatherings',\n",
    "'behavioral_outside_home','behavioral_wash_hands','behavioral_touch_face','doctor_recc_h1n1','doctor_recc_seasonal','chronic_med_condition',\n",
    "'child_under_6_months','health_worker','health_insurance','opinion_h1n1_vacc_effective','opinion_h1n1_sick_from_vacc','opinion_h1n1_risk',\n",
    "'opinion_seas_sick_from_vacc','household_adults','household_children']\n",
    "\n",
    "# Apply standardization on the numerical features\n",
    "for i in num_cols:\n",
    "    \n",
    "    # Fit the scaler on the training data column\n",
    "    scale_h1n1 = StandardScaler().fit(X_train_stand_h1n1[[i]])\n",
    "    scale_seasonal = StandardScaler().fit(X_train_stand_seasonal[[i]])\n",
    "    \n",
    "    # Transform the training data column\n",
    "    X_train_stand_h1n1[i] = scale_h1n1.transform(X_train_stand_h1n1[[i]])\n",
    "    X_train_stand_seasonal[i] = scale_seasonal.transform(X_train_stand_seasonal[[i]])\n",
    "    \n",
    "    # Transform the testing data column\n",
    "    X_test_stand_h1n1[i] = scale_h1n1.transform(X_test_stand_h1n1[[i]])\n",
    "    X_test_stand_seasonal[i] = scale_seasonal.transform(X_test_stand_seasonal[[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
